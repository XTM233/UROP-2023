{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Masked Language Modeling\n",
    "\n",
    "This notebook describes how one can pre-train their own AntiBERTa model using the HuggingFace framework. As a demo, we've included the tokenizer we've used, and 1% of the sequences that we used in our training, validation, and test sets of the paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup of all the things we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some imports \n",
    "from transformers import (\n",
    "    RobertaConfig,\n",
    "    RobertaTokenizer,\n",
    "    RobertaForMaskedLM,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from datasets import load_dataset\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialise the tokeniser\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"antibody-tokenizer\")\n",
    "\n",
    "# Initialise the data collator, which is necessary for batching\n",
    "collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO change this to import\n",
    "def txt2list(filepath):\n",
    "    \"\"\"\n",
    "    load text files, with each line as an element of list\n",
    "    Args:\n",
    "        filepath: a string indicating the path of a text file with each line is a literal tuple\n",
    "    Outputs:\n",
    "        lst: a list of tuple\n",
    "    \"\"\"\n",
    "    lst = []\n",
    "    f = open(filepath, \"r\")\n",
    "    Lines = f.readlines()\n",
    "    for line in Lines:\n",
    "        lst.append(eval(line))\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO delete test files\n",
    "# TODO job array runs afterok this script    \n",
    "# TODO export with error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify which round of training is this\n",
    "f = open(\"./index.txt\", \"r\")\n",
    "round_ind = int(f.readline())\n",
    "\n",
    "current_train_array = txt2list(f\"./round_{round_ind}.txt\")\n",
    "ind = int(os.environ[\"PBS_ARRAY_INDEX\"])-1\n",
    "n_head,n_layer,random_ind = current_train_array[ind] # will raise error if 0 inplaced \n",
    "random_seed = random_ind + 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_FASTA(filename):\n",
    "    count = 0\n",
    "    current_seq = ''\n",
    "    all_seqs = []\n",
    "    with open(filename,'r') as f:\n",
    "        for line in f:\n",
    "            if line[0] == '>':\n",
    "                all_seqs.append(current_seq)\n",
    "                current_seq = ''\n",
    "            else:\n",
    "                current_seq+=line[:-1]\n",
    "                count+=1\n",
    "        all_seqs.append(current_seq)\n",
    "        #all_seqs=np.array(map(lambda x: [aadict[y] for y in x],all_seqs[1:]),dtype=int,order=\"c\")\n",
    "    return all_seqs    \n",
    "\n",
    "# I aligned and prepared a set of ~3000 antibodies from SabDab\n",
    "name_fasta='sabdab_heavy.txt'\n",
    "seqs_al  =load_FASTA(name_fasta)[1:]\n",
    "f = open(\"sabdab_heavy_pos.txt\", \"r\")\n",
    "out = f.read()\n",
    "imgt_num=out.splitlines()\n",
    "\n",
    "## positions taken from IMGT templates ##\n",
    "b_cdr1 = imgt_num.index('27')\n",
    "e_cdr1 = imgt_num.index('38')\n",
    "\n",
    "b_cdr2 = imgt_num.index('56')\n",
    "e_cdr2 = imgt_num.index('65')\n",
    "\n",
    "b_cdr3 = imgt_num.index('105')\n",
    "e_cdr3 = imgt_num.index('117')\n",
    "    \n",
    "seqs=[]\n",
    "for s in range(len(seqs_al)):\n",
    "    seqs.append(''.join([seqs_al[s][i] for i in range(len(seqs_al[s])) if seqs_al[s][i]!='-']))\n",
    "    \n",
    "# print('average seq length original dataset')\n",
    "# lens = [len(s) for s in seqs]\n",
    "# print(np.mean(lens))\n",
    "# print('average seq length aligned dataset')\n",
    "# lens = [len(s) for s in seqs_al]\n",
    "# print(np.mean(lens))\n",
    "# NA= int(np.mean(lens))\n",
    "# print('length original dataset')\n",
    "# print(len(seqs))\n",
    "# print('length aligned dataset')\n",
    "# print(len(seqs_al))\n",
    "\n",
    "## here I verify that anarci can chop amino acids but only at the end##\n",
    "# '''\n",
    "# indices0 = file[file.columns[0]].values\n",
    "# indices = [int(ii[1:]) for ii in indices0]\n",
    "# final_indices = []\n",
    "# final_indices_al = []\n",
    "# for s in range(len(indices)):\n",
    "#     if abs(len(seqs[indices[s]])-len([seqs_al[s][p] for p in range(len(seqs_al[s])) if seqs_al[s][p]!='-'])) < 2:\n",
    "#         final_indices.append(indices[s])\n",
    "#         final_indices_al.append(s)\n",
    "# '''\n",
    "\n",
    "# Mb= len(seqs_al)\n",
    "# final_indices_al=list(np.arange(len(seqs_al)))\n",
    "# final_indices=list(np.arange(len(seqs)))\n",
    "        \n",
    "# m=3\n",
    "# print('example original seq')\n",
    "# print(seqs[final_indices[m]])\n",
    "# print('example aligned seq')\n",
    "# print(seqs_al[final_indices_al[m]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# write datasets into files if not already done\n",
    "split_ratio = [0.8,0.1,0.1]\n",
    "def write2file(st, name):\n",
    "    with open(\"./assets/\"+name+'.txt', 'w') as f:\n",
    "        for line in st:\n",
    "            f.write(line+\"\\n\")\n",
    "if not os.path.isfile(\"./assets/train_small.txt\"):\n",
    "    print(\"spliting\")\n",
    "    N = len(seqs)\n",
    "    np.random.seed(random_seed)\n",
    "    split_indices = np.random.permutation(N)\n",
    "    train_indices = split_indices[:int(N*split_ratio[0])]\n",
    "    # print(train_indices)\n",
    "    val_indices = split_indices[int(N*split_ratio[0]):int(N*((split_ratio[0])+split_ratio[1]))]\n",
    "    test_indices = split_indices[int(N*((split_ratio[0])+split_ratio[1])):]\n",
    "    seqs = np.array(seqs)\n",
    "    train_seqs = seqs[train_indices]\n",
    "    val_seqs = seqs[val_indices]\n",
    "    test_seqs = seqs[test_indices]\n",
    "    write2file(train_seqs, \"train_small\")\n",
    "    write2file(val_seqs, \"val_small\")\n",
    "    write2file(test_seqs, \"test_small\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-8cfdf19d33d34322\n",
      "Reusing dataset text (C:\\Users\\XTM23\\.cache\\huggingface\\datasets\\text\\default-8cfdf19d33d34322\\0.0.0\\e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "973cdf4ce0144be3a6b486dea4126262",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "249ccc78e748425c8dc51983828e31d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "229b5b45046e4c8b9d1725dd5e63d478",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text_datasets = {\n",
    "    \"train\": ['assets/train_small.txt'],\n",
    "    \"eval\": ['assets/val_small.txt'],\n",
    "    \"test\": ['assets/test_small.txt']\n",
    "}\n",
    "\n",
    "dataset = load_dataset(\"text\", data_files=text_datasets)\n",
    "\n",
    "tokenized_dataset = dataset.map(\n",
    "    lambda z: tokenizer(\n",
    "        z[\"text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=150,\n",
    "        return_special_tokens_mask=True,\n",
    "    ),\n",
    "    batched=True,\n",
    "    num_proc=1,\n",
    "    remove_columns=[\"text\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimate memory required to choose an optimal batch size to fully utilise memory\n",
    "# def count_parameters(model: torch.nn.Module) -> int:\n",
    "#     \"\"\" Returns the number of learnable parameters for a PyTorch model \"\"\"\n",
    "#     return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "# count_parameters(model) # return 7818265 for 1 hidden layer\n",
    "\n",
    "max_memory_per_batch = 7818265*n_layer*6 # consider mixed precision, 4*1.5 bytes for each parameter\n",
    "n = 24*1024**3/max_memory_per_batch\n",
    "batch_s = int(2**(np.floor(np.log2(n)))) # get a nearest power 2 as batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"-\".join([str(n_layer).zfill(2),\n",
    "                       str(n_head).zfill(2),\n",
    "                       str(random_ind).zfill(2)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the cofigurations we've used for pre-training.\n",
    "antiberta_config = {\n",
    "    #\"num_hidden_layers\": 12,\n",
    "    \"num_hidden_layers\": n_layer,\n",
    "    \"num_attention_heads\": n_head,\n",
    "    #\"num_attention_heads\": 12,\n",
    "    \"hidden_size\": 768,\n",
    "    \"d_ff\": 3072, # feed-forward dimension (possible to change?)\n",
    "    \"vocab_size\": 25, # 20 aa + 5 symbols, including masked, start-end\n",
    "    \"max_len\": 150,\n",
    "    \"max_position_embeddings\": 152, #?\n",
    "    \"batch_size\": batch_s, # params to explore\n",
    "    \"max_steps\": 225000, # params to explore\n",
    "    \"weight_decay\": 0.01, # params to explore\n",
    "    \"peak_learning_rate\": 0.0001, # params to explore\n",
    "    \"labels\":torch\n",
    "}\n",
    "\n",
    "# Initialise the model\n",
    "model_config = RobertaConfig(\n",
    "    vocab_size=antiberta_config.get(\"vocab_size\"),\n",
    "    hidden_size=antiberta_config.get(\"hidden_size\"),\n",
    "    max_position_embeddings=antiberta_config.get(\"max_position_embeddings\"),\n",
    "    num_hidden_layers=antiberta_config.get(\"num_hidden_layers\", 12),\n",
    "    num_attention_heads=antiberta_config.get(\"num_attention_heads\", 12),\n",
    "    type_vocab_size=1,\n",
    "    output_attentions=True\n",
    ")\n",
    "model = RobertaForMaskedLM(model_config)\n",
    "\n",
    "steps=50 #greater save steps, faster training\n",
    "# construct training arguments\n",
    "# Huggingface uses a default seed of 42\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"test\",\n",
    "    overwrite_output_dir=True,\n",
    "    per_device_train_batch_size=antiberta_config.get(\"batch_size\", 32),\n",
    "    per_device_eval_batch_size=antiberta_config.get(\"batch_size\", 32),\n",
    "    max_steps=antiberta_config.get(\"max_steps\", 12),\n",
    "    #save_steps=2500,\n",
    "    save_steps=steps,\n",
    "    eval_steps = steps,\n",
    "    logging_steps= steps, # params to explore\n",
    "    adam_beta2=0.98, # params to explore\n",
    "    adam_epsilon=1e-6, # params to explore\n",
    "    weight_decay=antiberta_config.get(\"weight_decay\", 12),\n",
    "    #warmup_steps = 10000, # params to explore\n",
    "    warmup_steps = 2, # params to explore\n",
    "    learning_rate=1e-4, # params to explore\n",
    "    save_total_limit = 3,\n",
    "    gradient_accumulation_steps=antiberta_config.get(\"gradient_accumulation_steps\", 8),\n",
    "    fp16=True, # True - CUDA\n",
    "    #bf16=True, # True - CUDA\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    load_best_model_at_end=True,\n",
    "    seed=random_seed\n",
    ")\n",
    "if args.eval_steps > args.max_steps:\n",
    "    print('Please change eval steps')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup of the HuggingFace Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model early stops if evaluation metric worsens for 10 eval steps\n",
    "MyCallback = EarlyStoppingCallback(10, 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[1;32m----> 2\u001b[0m     model\u001b[39m=\u001b[39mmodel,\n\u001b[0;32m      3\u001b[0m     args\u001b[39m=\u001b[39margs,\n\u001b[0;32m      4\u001b[0m     data_collator\u001b[39m=\u001b[39mcollator,\n\u001b[0;32m      5\u001b[0m     train_dataset\u001b[39m=\u001b[39mtokenized_dataset[\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[0;32m      6\u001b[0m     eval_dataset\u001b[39m=\u001b[39mtokenized_dataset[\u001b[39m\"\u001b[39m\u001b[39meval\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m# TODO inject tokenizer in trainer\u001b[39;00m\n\u001b[0;32m      7\u001b[0m )\n\u001b[0;32m      8\u001b[0m trainer\u001b[39m.\u001b[39madd_callback(MyCallback)\n\u001b[0;32m      9\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(os\u001b[39m.\u001b[39mlistdir(\u001b[39m\"\u001b[39m\u001b[39m./test\u001b[39m\u001b[39m\"\u001b[39m)) \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m     10\u001b[0m     \u001b[39m# check whether there is existing checkpoint\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    data_collator=collator,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"eval\"] # TODO inject tokenizer in trainer\n",
    ")\n",
    "trainer.add_callback(MyCallback)\n",
    "if len(os.listdir(\"./test\")) != 0:\n",
    "    # check whether there is existing checkpoint\n",
    "    trainer.train(resume_from_checkpoint=True)\n",
    "else:\n",
    "    trainer.train()\n",
    "\n",
    "trainer.save_model(\"./model/\"+model_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
