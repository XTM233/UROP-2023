{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Masked Language Modeling\n",
    "\n",
    "This notebook describes how one can pre-train their own AntiBERTa model using the HuggingFace framework. As a demo, we've included the tokenizer we've used, and 1% of the sequences that we used in our training, validation, and test sets of the paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup of all the things we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\XTM23\\miniconda3\\envs\\antiberv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Some imports \n",
    "from transformers import (\n",
    "    RobertaConfig,\n",
    "    RobertaTokenizer,\n",
    "    RobertaForMaskedLM,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "from datasets import load_dataset\n",
    "import os\n",
    "import torch\n",
    "\n",
    "# Initialise the tokeniser\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"antibody-tokenizer\")\n",
    "\n",
    "# Initialise the data collator, which is necessary for batching\n",
    "collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-842c374d83e54763\n",
      "Reusing dataset text (C:\\Users\\XTM23\\.cache\\huggingface\\datasets\\text\\default-842c374d83e54763\\0.0.0\\e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5)\n",
      "Parameter 'function'=<function <lambda> at 0x00000239B3109DC0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 1151/1151 [08:10<00:00,  2.35ba/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 135/135 [00:53<00:00,  2.51ba/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 72/72 [00:31<00:00,  2.32ba/s]\n"
     ]
    }
   ],
   "source": [
    "# this is a slice of the original dataset\n",
    "text_datasets = {\n",
    "    \"train\": ['assets/train-slice.txt'],\n",
    "    \"eval\": ['assets/val-slice.txt'],\n",
    "    \"test\": ['assets/test-slice.txt']\n",
    "}\n",
    "\n",
    "dataset = load_dataset(\"text\", data_files=text_datasets)\n",
    "\n",
    "tokenized_dataset = dataset.map(\n",
    "    lambda z: tokenizer(\n",
    "        z[\"text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=150,\n",
    "        return_special_tokens_mask=True,\n",
    "    ),\n",
    "    batched=True,\n",
    "    num_proc=1,\n",
    "    remove_columns=[\"text\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(134778, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset[\"eval\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the cofigurations we've used for pre-training.\n",
    "antiberta_config = {\n",
    "    #\"num_hidden_layers\": 12,\n",
    "    \"num_hidden_layers\": 12,\n",
    "    \"num_attention_heads\": 12,\n",
    "    #\"num_attention_heads\": 12,\n",
    "    \"hidden_size\": 768,\n",
    "    \"d_ff\": 3072, # feed-forward dimension (possible to change?)\n",
    "    \"vocab_size\": 25, # 20 aa + 5 symbols, including masked, start-end\n",
    "    \"max_len\": 150,\n",
    "    \"max_position_embeddings\": 152, #?\n",
    "    \"batch_size\": 96, # params to explore\n",
    "    \"max_steps\": 100000, # params to explore\n",
    "    #\"max_steps\": 225000, # params to explore\n",
    "    \"weight_decay\": 0.01, # params to explore\n",
    "    \"peak_learning_rate\": 0.0001, # params to explore\n",
    "    \"labels\":torch\n",
    "}\n",
    "\n",
    "# Initialise the model\n",
    "model_config = RobertaConfig(\n",
    "    vocab_size=antiberta_config.get(\"vocab_size\"),\n",
    "    hidden_size=antiberta_config.get(\"hidden_size\"),\n",
    "    max_position_embeddings=antiberta_config.get(\"max_position_embeddings\"),\n",
    "    num_hidden_layers=antiberta_config.get(\"num_hidden_layers\", 12),\n",
    "    num_attention_heads=antiberta_config.get(\"num_attention_heads\", 12),\n",
    "    type_vocab_size=1,\n",
    ")\n",
    "model = RobertaForMaskedLM(model_config)\n",
    "\n",
    "steps=50 #greater save steps, faster training\n",
    "# construct training arguments\n",
    "# Huggingface uses a default seed of 42\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"C:/Users/XTM23/Documents/antiberta/test\",\n",
    "    overwrite_output_dir=True,\n",
    "    per_device_train_batch_size=antiberta_config.get(\"batch_size\", 32),\n",
    "    per_device_eval_batch_size=antiberta_config.get(\"batch_size\", 32),\n",
    "    max_steps=antiberta_config.get(\"max_steps\", 12),\n",
    "    #save_steps=2500,\n",
    "    save_steps=steps,\n",
    "    eval_steps = steps,\n",
    "    logging_steps= steps, # params to explore\n",
    "    adam_beta2=0.98, # params to explore\n",
    "    adam_epsilon=1e-6, # params to explore\n",
    "    weight_decay=antiberta_config.get(\"weight_decay\", 12),\n",
    "    #warmup_steps = 10000, # params to explore\n",
    "    warmup_steps = 2, # params to explore\n",
    "    learning_rate=1e-4, # params to explore\n",
    "    save_total_limit = 3,\n",
    "    no_cuda=True,\n",
    "    gradient_accumulation_steps=antiberta_config.get(\"gradient_accumulation_steps\", 1),\n",
    "    #fp16=True, # True - CUDA\n",
    "    #bf16=True, # True - CUDA\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    load_best_model_at_end=True,\n",
    "    seed=40\n",
    ")\n",
    "if args.eval_steps > args.max_steps:\n",
    "    print('Please change eval steps')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup of the HuggingFace Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    data_collator=collator,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"eval\"]\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "model_name = \"-\".join([\"big\",str(antiberta_config[\"num_hidden_layers\"]),\n",
    "                       str(antiberta_config[\"num_attention_heads\"]),\n",
    "                       str(args.max_steps)])\n",
    "trainer.save_model(\"./model/\"+model_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:antiberv]",
   "language": "python",
   "name": "conda-env-antiberv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
